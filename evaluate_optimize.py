# ========================
# æ–‡ä»¶ï¼ševaluate.pyï¼ˆé’ˆå¯¹å¬å›ç‡è¯„ä¼°çš„ä¿®è®¢ï¼‰
# å…³é”®æ”¹åŠ¨ï¼š
# - å°† ROUGE ä½¿ç”¨ recallï¼ˆè€Œé f1ï¼‰è¯„ä¼°è¦†ç›–ç‡
# - è¾“å‡ºæ›´æ¸…æ™°çš„å¬å›æŒ‡æ ‡ï¼Œä¾¿äºå¯¹ç…§ 89% ç›®æ ‡
# ========================

import json
import os
from tqdm import tqdm
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from bert_score import score as bert_score

# æ³¨æ„ï¼šæŒ‰ä½ çš„å®é™…æ–‡ä»¶åå¯¼å…¥
from mylangchain_llm_optim_optimize import create_qa_system

smooth = SmoothingFunction().method1
# use_stemmer=True å¯¹è‹±æ–‡æ›´æœ‰æ•ˆï¼›ä¸­æ–‡å¯ä¿æŒ False æˆ–ä½¿ç”¨åˆ†è¯åå†ç®—
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)


def evaluate_answer(predicted, ground_truth):
    predicted = (predicted or "").strip()
    ground_truth = (ground_truth or "").strip()

    exact_match = int(predicted.lower() == ground_truth.lower())
    contains_score = int(ground_truth.lower() in predicted.lower() or predicted.lower() in ground_truth.lower())

    reference = [ground_truth.split()]
    hypothesis = predicted.split()
    bleu = sentence_bleu(reference, hypothesis, smoothing_function=smooth) if hypothesis else 0.0

    rouge = scorer.score(ground_truth, predicted)
    # é‡‡ç”¨å¬å›ç‡ï¼ˆrecallï¼‰ä½œä¸ºâ€œçŸ¥è¯†å¬å›ç‡â€çš„ä¸»è¦æŒ‡æ ‡
    rouge1 = rouge['rouge1'].recall
    rouge2 = rouge['rouge2'].recall
    rougeL = rouge['rougeL'].recall

    P, R, F1 = bert_score([predicted], [ground_truth], lang="zh", verbose=False)
    bert_f1 = float(F1[0])

    return {
        "exact_match": exact_match,
        "contains": contains_score,
        "bleu": bleu,
        "rouge1": rouge1,
        "rouge2": rouge2,
        "rougeL": rougeL,
        "bertscore": bert_f1,
    }


def evaluate_model(model_info, questions):
    print(f"\nğŸ§ª æ­£åœ¨è¯„ä¼°æ¨¡å‹: {model_info['name']}")
    qa_system = create_qa_system(
        api_base=model_info["api_base"],
        api_key=model_info["api_key"],
        model_name=model_info["model_name"],
        temperature=0.1,
        top_k=10
    )

    model_results = []
    for item in tqdm(questions, desc=f"æ¨¡å‹ {model_info['name']}"):
        question = item["question"]
        answer = item["answer"]
        print(f"ç­”æ¡ˆï¼š{answer}")
        predicted = qa_system.query(question)
        print(f"é¢„æµ‹ï¼š{predicted}")
        metrics = evaluate_answer(predicted, answer)
        metrics.update({
            "model": model_info["name"],
            "question": question,
            "predicted": predicted,
            "ground_truth": answer,
        })
        model_results.append(metrics)
    return model_results


def summarize_report(all_results, save_csv=False):
    from collections import defaultdict
    import csv

    stats = defaultdict(list)
    for r in all_results:
        stats[r["model"]].append(r)

    print("\nğŸ“Š æ¨¡å‹è¯„ä¼°æ±‡æ€»ï¼š")
    summary = []
    for model, records in stats.items():
        total = len(records)
        avg = lambda k: sum(r[k] for r in records) / total
        row = {
            "model": model,
            "samples": total,
            "exact_match": avg("exact_match"),
            "contains": avg("contains"),
            "bleu": avg("bleu"),
            "rouge1": avg("rouge1"),
            "rouge2": avg("rouge2"),
            "rougeL": avg("rougeL"),
            "bertscore": avg("bertscore"),
        }
        summary.append(row)

        print(f"\nğŸ§  {model} - æ ·æœ¬æ•°: {total}")
        for k in ["exact_match","contains","rouge1","rouge2","rougeL","bleu","bertscore"]:
            print(f"{k}: {row[k]:.4f}")

    os.makedirs("results", exist_ok=True)
    with open("results/report.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    if save_csv and summary:
        import csv
        with open("results/report.csv", "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=list(summary[0].keys()))
            writer.writeheader()
            writer.writerows(summary)
    return summary


if __name__ == "__main__":
    models = [
        {
            "name": "deepseek-chat",
            "model_name": "deepseek-chat",
            "api_base": "https://api.deepseek.com/v1",
            "api_key": "sk-aa9ac612575f4cffb71e8e2e537a4e50"
        }
    ]

    with open("data/questions.json", "r", encoding="utf-8") as f:
        qa_data = json.load(f)

    all_model_results = []
    for m in models:
        all_model_results.extend(evaluate_model(m, qa_data))

    summarize_report(all_model_results, save_csv=True)