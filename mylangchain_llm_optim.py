import os
import json
import time
import requests  # ç”¨äºAPIå¥åº·æ£€æŸ¥
import torch  # ç”¨äºæ£€æµ‹GPU
from tqdm import tqdm
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.schema import Document
import shutil  # ç”¨äºè‡ªåŠ¨åˆ é™¤æ—§ç´¢å¼•
import os

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"


# --- è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå°†åˆ—è¡¨åˆ†æ‰¹ ---
def batch_list(data: list, batch_size: int):
    """å°†åˆ—è¡¨åˆ‡åˆ†ä¸ºæŒ‡å®šå¤§å°çš„æ‰¹æ¬¡"""
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]


# --- æ ¸å¿ƒç±»ï¼šé—®ç­”ç³»ç»Ÿ ---
class IntelligentQASystem:
    def __init__(self, bidding_store, enterprise_store, llm, prompt):
        self.bidding_retriever = bidding_store.as_retriever(search_kwargs={"k": 5})
        self.enterprise_retriever = enterprise_store.as_retriever(search_kwargs={"k": 5})
        self.llm = llm
        self.prompt = prompt

    def query(self, question):
        """
        æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„é—®ç­”æµç¨‹
        """
        try:
            # ä½¿ç”¨ .invoke() æ›¿ä»£å·²å¼ƒç”¨çš„ .get_relevant_documents()
            bidding_docs = self.bidding_retriever.invoke(question)
            enterprise_docs = self.enterprise_retriever.invoke(question)

            context_bidding = "\n\n".join([doc.page_content for doc in bidding_docs])
            context_enterprise = "\n\n".join([doc.page_content for doc in enterprise_docs])

            inputs = {
                "context_bidding": context_bidding,
                "context_enterprise": context_enterprise,
                "question": question
            }
            chain = self.prompt | self.llm
            response = chain.invoke(inputs)
            return response.content
        except Exception as e:
            return f"æŸ¥è¯¢å¤±è´¥: {str(e)}. è¯·æ£€æŸ¥åµŒå…¥æ¨¡å‹æˆ–APIè¿æ¥ã€‚"


# --- æ ¸å¿ƒå‡½æ•°ï¼šé‡å»ºFAISSç´¢å¼• ---
def rebuild_index(json_path, index_path, embeddings_model):
    """
    è¯»å–JSONæ–‡ä»¶ï¼Œåˆ›å»ºä¼˜åŒ–çš„æ–‡æ¡£å†…å®¹ï¼Œå¹¶æ„å»ºFAISSç´¢å¼•
    """
    print(f"å¼€å§‹é‡å»ºç´¢å¼•: {index_path}...")
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)["data"]

    docs = []
    print(f"æ­£åœ¨å‡†å¤‡æ›´é€‚åˆåµŒå…¥çš„æ–‡æ¡£å†…å®¹...")
    for item in tqdm(data, desc=f"æ­£åœ¨å¤„ç† {os.path.basename(json_path)}"):
        content_str = ""
        if isinstance(item, dict):
            for key, value in item.items():
                if value is not None and value != '':
                    content_str += f"{key}: {value}\n"
        else:
            content_str = str(item)

        doc = Document(
            page_content=content_str.strip(),
            metadata={"source_data": json.dumps(item, ensure_ascii=False)}
        )
        docs.append(doc)

    texts = [doc.page_content for doc in docs]
    metadatas = [doc.metadata for doc in docs]
    batch_size = 64

    print(f"å¼€å§‹ä½¿ç”¨æœ¬åœ°æ¨¡å‹åˆ†æ‰¹ç”Ÿæˆå‘é‡ (æ¯æ‰¹ {batch_size} æ¡)...")
    all_embeddings = []
    for batch_texts in tqdm(list(batch_list(texts, batch_size)), desc=f"æ­£åœ¨åµŒå…¥ {os.path.basename(json_path)}"):
        batch_embeddings = embeddings_model.embed_documents(batch_texts)
        all_embeddings.extend(batch_embeddings)

    if not all_embeddings:
        raise ValueError("å‘é‡åµŒå…¥å¤±è´¥ï¼Œæœªèƒ½ç”Ÿæˆä»»ä½•å‘é‡ã€‚")

    text_embedding_pairs = list(zip(texts, all_embeddings))

    print("å‘é‡ç”Ÿæˆå®Œæ¯•ï¼Œæ­£åœ¨æ„å»ºFAISSç´¢å¼•...")
    store = FAISS.from_embeddings(text_embedding_pairs, embeddings_model, metadatas=metadatas)
    store.save_local(index_path)
    print(f"ç´¢å¼• {index_path} é‡å»ºå¹¶ä¿å­˜æˆåŠŸï¼")
    return store


# --- æ ¸å¿ƒå‡½æ•°ï¼šåˆ›å»ºé—®ç­”ç³»ç»Ÿå®ä¾‹ ---
def create_qa_system(api_base=None, api_key=None, model_name="gpt-3.5-turbo", temperature=0.1, **kwargs):
    """
    æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼Œåˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªé—®ç­”ç³»ç»Ÿå®ä¾‹
    """

    # def check_api_health(base_url, key):
    #     print("æ­£åœ¨æ£€æŸ¥LLM APIè¿æ¥...")
    #     try:
    #         headers = {"Authorization": f"Bearer {key}"}
    #         response = requests.get(f"{base_url}/models", headers=headers, timeout=10)
    #         if response.status_code != 200:
    #             raise ValueError(f"APIçŠ¶æ€ç : {response.status_code} - {response.text}")
    #         print("LLM APIè¿æ¥æˆåŠŸï¼")
    #     except Exception as e:
    #         raise ConnectionError(f"LLM APIè¿æ¥å¤±è´¥: {str(e)}. è¯·æ£€æŸ¥API_BASEã€API_KEYå’Œç½‘ç»œã€‚")
    #
    # if api_base:
    #     check_api_health(api_base, api_key)

    print("æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°åµŒå…¥æ¨¡å‹ BAAI/bge-base-zh-v1.5...")
    print("ï¼ˆé¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œå¤§å°çº¦400-500MBï¼Œè¯·è€å¿ƒç­‰å¾…...ï¼‰")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"æ£€æµ‹åˆ°åµŒå…¥æ¨¡å‹å°†ä½¿ç”¨çš„è®¾å¤‡: {device}")

    # embeddings = HuggingFaceBgeEmbeddings(
    #     model_name="BAAI/bge-base-zh-v1.5",
    #     model_kwargs={"device": device},
    #     encode_kwargs={"normalize_embeddings": True}
    # )

    embeddings = HuggingFaceBgeEmbeddings(
        model_name="E:\\ai\\models\\bge-base-zh-v1.5",
        model_kwargs={"device": device},
        encode_kwargs={"normalize_embeddings": True}
    )
    print("æœ¬åœ°åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    bidding_index_path = "./data/bidding_index"
    enterprise_index_path = "./data/enterprise_data"
    bidding_json_path = "./data/bidding_index.json"
    enterprise_json_path = "./data/enterprise_data.json"

    for index_path, json_path in [(bidding_index_path, bidding_json_path),
                                  (enterprise_index_path, enterprise_json_path)]:
        try:
            FAISS.load_local(index_path, embeddings=embeddings, allow_dangerous_deserialization=True)
            print(f"{os.path.basename(index_path)} ç´¢å¼•åŠ è½½æˆåŠŸï¼Œæ— éœ€é‡å»ºï¼")
        except Exception as e:
            print(f"{os.path.basename(index_path)} ç´¢å¼•åŠ è½½å¤±è´¥: {e}ï¼Œå¼€å§‹é‡å»º...")
            rebuild_index(json_path, index_path, embeddings)

    bidding_store = FAISS.load_local(bidding_index_path, embeddings, allow_dangerous_deserialization=True)
    enterprise_store = FAISS.load_local(enterprise_index_path, embeddings, allow_dangerous_deserialization=True)

    llm = ChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        openai_api_base=api_base,
        openai_api_key=api_key
    )

    # from dotenv import load_dotenv, find_dotenv
    #
    # _ = load_dotenv(find_dotenv())
    # llm = ChatOpenAI(model="gpt-3.5-turbo")

    prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›æŠ•æ ‡é‡‡è´­AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„â€œæ£€ç´¢åˆ°çš„æ‹›æ ‡æ–‡æ¡£â€å’Œâ€œæ£€ç´¢åˆ°çš„ä¼ä¸šæ–‡æ¡£â€ä¸­çš„ä¿¡æ¯ï¼Œæ¥å…¨é¢ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1.  ç­”æ¡ˆå¿…é¡»å®Œå…¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç¦æ­¢ç¼–é€ æˆ–ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†ã€‚
2.  å¦‚æœæä¾›çš„ä¿¡æ¯è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ¸…æ™°ã€æœ‰æ¡ç†åœ°ç»„ç»‡ç­”æ¡ˆã€‚
3.  å¦‚æœæä¾›çš„ä¿¡æ¯ä¸åŒ…å«ä¸é—®é¢˜ç›¸å…³çš„ä»»ä½•å†…å®¹ï¼Œè¯·æ˜ç¡®å›ç­”ï¼šâ€œæ ¹æ®ç°æœ‰èµ„æ–™ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚â€
4.  åœ¨å›ç­”æ—¶ï¼Œä¸è¦æåŠâ€œæ ¹æ®æ£€ç´¢åˆ°çš„ä¿¡æ¯â€æˆ–â€œæ ¹æ®ä¸Šä¸‹æ–‡â€ç­‰è¯è¯­ï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚

---
[æ£€ç´¢åˆ°çš„æ‹›æ ‡æ–‡æ¡£]
{context_bidding}
---
[æ£€ç´¢åˆ°çš„ä¼ä¸šæ–‡æ¡£]
{context_enterprise}
---

[ç”¨æˆ·é—®é¢˜]
{question}

[ä½ çš„å›ç­”]
"""
    PROMPT = PromptTemplate(template=prompt_template,
                            input_variables=["context_bidding", "context_enterprise", "question"])

    return IntelligentQASystem(bidding_store, enterprise_store, llm, PROMPT)


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # ========== é…ç½®åŒºåŸŸ ==========
    # LLM (å¤§è¯­è¨€æ¨¡å‹) çš„é…ç½®
    # API_KEY = "sk-aa9ac612575f4cffb71e8e2e537a4e50"  # ï¼ï¼ï¼è¯·æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®æœ‰æ•ˆå¯†é’¥ï¼ï¼ï¼
    # API_BASE = "https://api.deepseek.com/v1"  # å¡«å†™ä½ çš„APIåœ°å€
    # MODEL_NAME = "deepseek-chat"

    # API_KEY = "sk-8c99bbe5be3344a888ddd08d14cc8d65"  # ï¼ï¼ï¼è¯·æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®æœ‰æ•ˆå¯†é’¥ï¼ï¼ï¼
    # API_BASE = "https://dashscope.aliyuncs.com/compatible-mode/v1"  # é˜¿é‡Œäº‘ APIåœ°å€
    # MODEL_NAME = "qwen-max"

    API_KEY = "sk-8c99bbe5be3344a888ddd08d14cc8d65"  # ï¼ï¼ï¼è¯·æ›¿æ¢ä¸ºæ‚¨çš„çœŸå®æœ‰æ•ˆå¯†é’¥ï¼ï¼ï¼
    API_BASE = "https://dashscope.aliyuncs.com/compatible-mode/v1"  # é˜¿é‡Œäº‘ APIåœ°å€ æœ‰å…è´¹é¢åº¦
    MODEL_NAME = "qwen-plus-0723"
    # ==============================

    print("\n" + "=" * 80)
    print("ğŸš¨ æ¬¢è¿ä½¿ç”¨æ™ºèƒ½æ‹›æŠ•æ ‡é‡‡è´­é—®ç­”ç³»ç»Ÿ")
    print("ğŸš¨ é‡è¦æç¤ºï¼šé¦–æ¬¡è¿è¡Œæˆ–æ›´æ¢æ•°æ®/æ¨¡å‹åï¼Œè¯·åŠ¡å¿…åˆ é™¤æ—§çš„FAISSç´¢å¼•ç›®å½•ï¼")
    print("=" * 80 + "\n")

    # è‡ªåŠ¨åˆ é™¤æ—§ç´¢å¼•ï¼ˆå»ºè®®é¦–æ¬¡è¿è¡Œæ—¶å–æ¶ˆæ³¨é‡Šï¼‰
    # for path_to_remove in ["./data/bidding_index", "./data/enterprise_data"]:
    #      if os.path.exists(path_to_remove):
    #          shutil.rmtree(path_to_remove)
    #          print(f"å·²è‡ªåŠ¨åˆ é™¤æ—§ç´¢å¼•ç›®å½•: {path_to_remove}")

    print(f"ä½¿ç”¨LLM: {MODEL_NAME}")
    print(f"ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹: BAAI/bge-base-zh-v1.5\n")

    try:
        qa_system = create_qa_system(
            api_base=API_BASE,
            api_key=API_KEY,
            model_name=MODEL_NAME,
            temperature=0.1,
        )
        print("\n" + "=" * 80)
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼å·²è¿›å…¥è¿ç»­é—®ç­”æ¨¡å¼ã€‚")
        print("=" * 80 + "\n")

        # --- æ–°å¢åŠŸèƒ½ï¼šå¾ªç¯æé—® ---
        while True:
            user_question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ (è¾“å…¥ 'é€€å‡º' æˆ– 'exit' æ¥ç»“æŸ): \n> ")
            if user_question.lower() in ["é€€å‡º", "exit"]:
                print("\næ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break

            if not user_question.strip():
                print("\n[ç³»ç»Ÿæç¤º] æ‚¨è¾“å…¥çš„é—®é¢˜ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
                continue

            print("\n[AIåŠ©æ‰‹] æ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å€™...")
            start_time = time.time()
            answer = qa_system.query(user_question)
            end_time = time.time()

            print(f"\n[AIåŠ©æ‰‹]:\n{answer}")
            print(f"\n(æœ¬æ¬¡å›ç­”è€—æ—¶: {end_time - start_time:.2f} ç§’)")
            print("\n" + "-" * 80 + "\n")

    except Exception as e:
        print(f"\n[è‡´å‘½é”™è¯¯] ç³»ç»Ÿåˆå§‹åŒ–æˆ–è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("ç¨‹åºå·²ç»ˆæ­¢ã€‚è¯·æ£€æŸ¥é…ç½®ã€ç½‘ç»œæˆ–ä¾èµ–é¡¹ã€‚")