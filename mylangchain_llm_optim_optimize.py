import os
import json
import time
import requests
import torch
from tqdm import tqdm
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_community.retrievers import BM25Retriever
from FlagEmbedding import FlagReranker  # ç”¨äºé‡æ’åº
import re

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# --- è¾…åŠ©å‡½æ•° ---
def batch_list(data: list, batch_size: int):
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]

def split_text(text, chunk_size=300, overlap=50):
    """ä¼˜åŒ–åˆ‡åˆ†ï¼šä¼˜å…ˆæŒ‰å¥/å­—æ®µåˆ‡å‰²ï¼Œå†åšæ»‘åŠ¨çª—å£"""
    # æŒ‰æ¢è¡Œå’Œå¥å·åˆ†å‰²
    sentences = re.split(r'(?<=[ã€‚ï¼›ï¼ï¼š\n])', text)
    chunks, current = [], ""
    for s in sentences:
        if len(current) + len(s) <= chunk_size:
            current += s
        else:
            chunks.append(current.strip())
            current = s
    if current:
        chunks.append(current.strip())

    # æ»‘åŠ¨çª—å£è¡¥å……
    final_chunks = []
    for c in chunks:
        if len(c) <= chunk_size:
            final_chunks.append(c)
        else:
            start = 0
            while start < len(c):
                end = min(len(c), start + chunk_size)
                final_chunks.append(c[start:end])
                start += chunk_size - overlap
    return final_chunks

def merge_results(vector_docs, keyword_docs, top_k=10):
    """åˆå¹¶å‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢ç»“æœå¹¶å»é‡"""
    all_docs = {doc.page_content: doc for doc in (vector_docs + keyword_docs)}
    return list(all_docs.values())[:top_k]

# --- æ ¸å¿ƒç±» ---
class IntelligentQASystem:
    def __init__(self, bidding_store, enterprise_store, llm, prompt, top_k=10, use_reranker=True):
        self.bidding_retriever = bidding_store.as_retriever(search_kwargs={"k": top_k*2})
        self.enterprise_retriever = enterprise_store.as_retriever(search_kwargs={"k": top_k*2})
        self.llm = llm
        self.prompt = prompt
        self.top_k = top_k
        self.use_reranker = use_reranker
        if use_reranker:
            print("åŠ è½½é‡æ’åºæ¨¡å‹ bge-reranker-large ...")
            self.reranker = FlagReranker(r"E:\ai\models\bge-reranker-large", use_fp16=True)
        else:
            self.reranker = None

    def expand_query(self, question):
        """ç”ŸæˆåŒä¹‰é—®æ³• + å…³é”®è¯æŠ½å–"""
        queries = [question]
        try:
            # åŒä¹‰é—®æ³•
            prompt = f"è¯·å¸®æˆ‘ç”Ÿæˆ3ä¸ªä¸ä»¥ä¸‹é—®é¢˜æ„æ€ç›¸åŒçš„ä¸åŒé—®æ³•ï¼š\n{question}"
            variants = self.llm.invoke(prompt).content.strip().split("\n")
            variants = [v.strip(" -") for v in variants if v.strip()]
            queries.extend(variants)

            # å…³é”®è¯æŠ½å–
            kw_prompt = f"è¯·ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–å‡ºæœ€æ ¸å¿ƒçš„å…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰ï¼š\n{question}"
            keywords = self.llm.invoke(kw_prompt).content.strip()
            if keywords and len(keywords) < 30:
                queries.append(keywords)
        except:
            pass
        return list(set(queries))

    def rerank_docs(self, docs, query):
        """å¯¹æ£€ç´¢ç»“æœé‡æ’åº"""
        if not self.reranker or not docs:
            return docs
        pairs = [(query, doc.page_content) for doc in docs]
        scores = self.reranker.compute_score(pairs)
        ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in ranked[:self.top_k]]

    def query(self, question):
        try:
            queries = self.expand_query(question)
            bidding_docs, enterprise_docs = [], []

            for q in queries:
                # å‘é‡æ£€ç´¢
                vec_bidding = self.bidding_retriever.invoke(q)
                vec_enterprise = self.enterprise_retriever.invoke(q)
                # BM25 æ£€ç´¢ï¼ˆåŠ å¤§èŒƒå›´ï¼‰
                bm25_bidding = BM25Retriever.from_documents(
                    list(self.bidding_retriever.vectorstore.docstore._dict.values()),
                    k=self.top_k*2
                ).invoke(q)
                bm25_enterprise = BM25Retriever.from_documents(
                    list(self.enterprise_retriever.vectorstore.docstore._dict.values()),
                    k=self.top_k*2
                ).invoke(q)
                # åˆå¹¶ç»“æœ
                bidding_docs.extend(merge_results(vec_bidding, bm25_bidding, self.top_k*2))
                enterprise_docs.extend(merge_results(vec_enterprise, bm25_enterprise, self.top_k*2))

            # å»é‡
            bidding_docs = list({doc.page_content: doc for doc in bidding_docs}.values())
            enterprise_docs = list({doc.page_content: doc for doc in enterprise_docs}.values())

            # é‡æ’åº
            bidding_docs = self.rerank_docs(bidding_docs, question)
            enterprise_docs = self.rerank_docs(enterprise_docs, question)

            context_bidding = "\n\n".join([doc.page_content for doc in bidding_docs])
            context_enterprise = "\n\n".join([doc.page_content for doc in enterprise_docs])

            inputs = {
                "context_bidding": context_bidding,
                "context_enterprise": context_enterprise,
                "question": question
            }
            chain = self.prompt | self.llm
            response = chain.invoke(inputs)
            return response.content
        except Exception as e:
            return f"æŸ¥è¯¢å¤±è´¥: {str(e)}. è¯·æ£€æŸ¥åµŒå…¥æ¨¡å‹æˆ–APIè¿æ¥ã€‚"

# --- æ„å»ºç´¢å¼• ---
def rebuild_index(json_path, index_path, embeddings_model):
    print(f"å¼€å§‹é‡å»ºç´¢å¼•: {index_path}...")
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)["data"]

    docs = []
    print(f"æ­£åœ¨å‡†å¤‡æ–‡æ¡£å†…å®¹å¹¶åˆ‡åˆ†...")
    for item in tqdm(data, desc=f"æ­£åœ¨å¤„ç† {os.path.basename(json_path)}"):
        content_str = ""
        if isinstance(item, dict):
            for key, value in item.items():
                if value:
                    content_str += f"{key}: {value}\n"
        else:
            content_str = str(item)

        # åˆ‡åˆ†é•¿æ–‡æœ¬
        for chunk in split_text(content_str.strip()):
            docs.append(Document(
                page_content=chunk,
                metadata={"source_data": json.dumps(item, ensure_ascii=False)}
            ))

    texts = [doc.page_content for doc in docs]
    metadatas = [doc.metadata for doc in docs]
    batch_size = 64

    print(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆå‘é‡ (æ¯æ‰¹ {batch_size} æ¡)...")
    all_embeddings = []
    for batch_texts in tqdm(list(batch_list(texts, batch_size)), desc=f"æ­£åœ¨åµŒå…¥ {os.path.basename(json_path)}"):
        batch_embeddings = embeddings_model.embed_documents(batch_texts)
        all_embeddings.extend(batch_embeddings)

    if not all_embeddings:
        raise ValueError("å‘é‡åµŒå…¥å¤±è´¥ã€‚")

    text_embedding_pairs = list(zip(texts, all_embeddings))
    print("å‘é‡ç”Ÿæˆå®Œæ¯•ï¼Œæ­£åœ¨æ„å»ºFAISSç´¢å¼•...")
    store = FAISS.from_embeddings(text_embedding_pairs, embeddings_model, metadatas=metadatas)
    store.save_local(index_path)
    print(f"ç´¢å¼• {index_path} é‡å»ºå¹¶ä¿å­˜æˆåŠŸï¼")
    return store

# --- åˆ›å»ºé—®ç­”ç³»ç»Ÿ ---
def create_qa_system(api_base=None, api_key=None, model_name="gpt-3.5-turbo", temperature=0.1, top_k=10, **kwargs):
    print("æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°åµŒå…¥æ¨¡å‹ BAAI/bge-base-zh-v1.5...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    embeddings = HuggingFaceBgeEmbeddings(
        model_name="E:\\ai\\models\\bge-base-zh-v1.5",
        model_kwargs={"device": device},
        encode_kwargs={"normalize_embeddings": True}
    )
    print("æœ¬åœ°åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    bidding_index_path = "./data/bidding_index"
    enterprise_index_path = "./data/enterprise_data"
    bidding_json_path = "./data/bidding_index.json"
    enterprise_json_path = "./data/enterprise_data.json"

    for index_path, json_path in [(bidding_index_path, bidding_json_path),
                                  (enterprise_index_path, enterprise_json_path)]:
        try:
            FAISS.load_local(index_path, embeddings=embeddings, allow_dangerous_deserialization=True)
            print(f"{os.path.basename(index_path)} ç´¢å¼•åŠ è½½æˆåŠŸï¼Œæ— éœ€é‡å»ºï¼")
        except:
            print(f"{os.path.basename(index_path)} ç´¢å¼•ç¼ºå¤±æˆ–æŸåï¼Œå¼€å§‹é‡å»º...")
            rebuild_index(json_path, index_path, embeddings)

    bidding_store = FAISS.load_local(bidding_index_path, embeddings, allow_dangerous_deserialization=True)
    enterprise_store = FAISS.load_local(enterprise_index_path, embeddings, allow_dangerous_deserialization=True)

    llm = ChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        openai_api_base=api_base,
        openai_api_key=api_key
    )

    prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›æŠ•æ ‡é‡‡è´­AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„â€œæ£€ç´¢åˆ°çš„æ‹›æ ‡æ–‡æ¡£â€å’Œâ€œæ£€ç´¢åˆ°çš„ä¼ä¸šæ–‡æ¡£â€ä¸­çš„ä¿¡æ¯ï¼Œæ¥å…¨é¢ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. ç­”æ¡ˆå¿…é¡»å®Œå…¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç¦æ­¢ç¼–é€ æˆ–ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†ã€‚
2. å¦‚æœæä¾›çš„ä¿¡æ¯è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ¸…æ™°ã€æœ‰æ¡ç†åœ°ç»„ç»‡ç­”æ¡ˆã€‚
3. å¦‚æœæä¾›çš„ä¿¡æ¯ä¸åŒ…å«ä¸é—®é¢˜ç›¸å…³çš„ä»»ä½•å†…å®¹ï¼Œè¯·æ˜ç¡®å›ç­”ï¼šâ€œæ ¹æ®ç°æœ‰èµ„æ–™ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚â€
4. åœ¨å›ç­”æ—¶ï¼Œä¸è¦æåŠâ€œæ ¹æ®æ£€ç´¢åˆ°çš„ä¿¡æ¯â€æˆ–â€œæ ¹æ®ä¸Šä¸‹æ–‡â€ç­‰è¯è¯­ï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚

---
[æ£€ç´¢åˆ°çš„æ‹›æ ‡æ–‡æ¡£]
{context_bidding}
---
[æ£€ç´¢åˆ°çš„ä¼ä¸šæ–‡æ¡£]
{context_enterprise}
---

[ç”¨æˆ·é—®é¢˜]
{question}

[ä½ çš„å›ç­”]
"""
    PROMPT = PromptTemplate(template=prompt_template,
                            input_variables=["context_bidding", "context_enterprise", "question"])

    return IntelligentQASystem(bidding_store, enterprise_store, llm, PROMPT, top_k=top_k, use_reranker=True)

# --- ä¸»ç¨‹åº ---
if __name__ == "__main__":
    API_KEY = "ä½ çš„API_KEY"
    API_BASE = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    MODEL_NAME = "qwen-plus-0723"

    print("ğŸš¨ æ¬¢è¿ä½¿ç”¨æ™ºèƒ½æ‹›æŠ•æ ‡é‡‡è´­é—®ç­”ç³»ç»Ÿ ğŸš¨")

    qa_system = create_qa_system(
        api_base=API_BASE,
        api_key=API_KEY,
        model_name=MODEL_NAME,
        temperature=0.1,
        top_k=10
    )

    while True:
        user_question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ (è¾“å…¥ 'é€€å‡º' æˆ– 'exit' æ¥ç»“æŸ): \n> ")
        if user_question.lower() in ["é€€å‡º", "exit"]:
            print("å†è§ï¼")
            break
        if not user_question.strip():
            continue
        print("[AIåŠ©æ‰‹] æ­£åœ¨æ€è€ƒä¸­...")
        start_time = time.time()
        answer = qa_system.query(user_question)
        end_time = time.time()
        print(f"\n[AIåŠ©æ‰‹]:\n{answer}")
        print(f"(è€—æ—¶: {end_time - start_time:.2f} ç§’)")
